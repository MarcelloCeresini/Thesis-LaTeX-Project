%% This BibTeX bibliography file was created using BibDesk.
%% https://bibdesk.sourceforge.io/

%% Created for Marcello Ceresini at 2024-03-13 18:26:34 +0100 


%% Saved with string encoding Unicode (UTF-8) 



@misc{brody_how_2022,
	abstract = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how\_attentive\_are\_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library.},
	annote = {Extracted Annotations (2/14/2024, 10:26:15 AM)
"GAT, every node attends to its neighbors given its own representation as the query." (Brody et al 2022:1)"ranking of the attention scores is unconditioned on the query node." (Brody et al 2022:1)"0 00 standard GAT" (Brody et al 2022:2)"all queries (q0 to 0 0 0 0 0 00nd mostly to the 8th key (k8)" (Brody et al 2022:2)"0 00 In contrast, GATv2 (Figure 1b) can actually compute dynamic attention, where every query has a different ranking of attention coefficients of the keys" (Brody et al 2022:2)"where a 2R2d0 , W 2Rd0 d are learned, and k denotes vector concatenation" (Brody et al 2022:3)"GAT computes a weighted average of the transformed features of the neighbor nodes" (Brody et al 2022:3)"ij W hj" (Brody et al 2022:3)"scoring function e :Rd Rd !R computes a score for every edge (j; i), which indicates the importance of the features of the neighbor j to the node i" (Brody et al 2022:3)"Attention is a mechanism for computing a distribution over a set of input key vectors, given an additional query vector" (Brody et al 2022:4)"If the attention function always weighs one key at least as much as any other key, unconditioned on the query, we say that this attention function is static" (Brody et al 2022:4)"attention). A (" (Brody et al 2022:4)"for every query i 2 [m]" (Brody et al 2022:4)"if for every f 2 F there exists a "highest scoring" key jf 2 [n]" (Brody et al 2022:4)"Note that dynamic and static attention are exclusive properties, but they are not complementary" (Brody et al 2022:4)"The learned parameter a can be written as a" (Brody et al 2022:4)"concatenation a = [a1 ka2 ] 2R2d0 such that a1 ; a2 2Rd0" (Brody et al 2022:5)"there exists a node jmax" (Brody et al 2022:5)"LeakyReLU a{\textgreater}W hi + a{\textgreater}W hj" (Brody et al 2022:5)"" (Brody et al 2022:5)"is maximal among all nodes" (Brody et al 2022:5)"uch that a{\textgreater}W hjmax" (Brody et al 2022:5)"2 maxotonicity of LeakyReLU and softmax, for every query node i 2 V , the node jmax also leads to the maximal value of its attention distribution" (Brody et al 2022:5)"The main problem in the standard GAT scoring function (Equation (2)) is that the learned layers W and a are applied consecutively, and thus can be collapsed into a single linear layer." (Brody et al 2022:5)"simply apply the a layer after the nonlinearity (LeakyReLU), and the W layer after the concatenation" (Brody et al 2022:5)"effectively applying an MLP to compute the score for each query-key pair" (Brody et al 2022:5)},
	author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
	date-added = {2024-03-13 18:25:48 +0100},
	date-modified = {2024-03-13 18:25:48 +0100},
	doi = {10.48550/arXiv.2105.14491},
	file = {arXiv.org Snapshot:/Users/marcelloceresini/Zotero/storage/AW9CJ828/2105.html:text/html;Brody et al_2022_How Attentive are Graph Attention Networks.pdf:/Users/marcelloceresini/Zotero/storage/QJ44LHKJ/Brody et al_2022_How Attentive are Graph Attention Networks.pdf:application/pdf},
	keywords = {Computer Science - Machine Learning, notion, ALREADY READ},
	month = jan,
	note = {arXiv:2105.14491 [cs]},
	publisher = {arXiv},
	title = {How {Attentive} are {Graph} {Attention} {Networks}?},
	url = {http://arxiv.org/abs/2105.14491},
	urldate = {2024-02-13},
	year = {2022},
	bdsk-url-1 = {http://arxiv.org/abs/2105.14491},
	bdsk-url-2 = {https://doi.org/10.48550/arXiv.2105.14491}}

@article{eivazi_physics-informed_2022,
	abstract = {Physics-informed neural networks (PINNs) are successful machine-learning methods for the solution and identification of partial differential equations. We employ PINNs for solving the Reynolds-averaged Navier--Stokes equations for incompressible turbulent flows without any specific model or assumption for turbulence and by taking only the data on the domain boundaries. We first show the applicability of PINNs for solving the Navier--Stokes equations for laminar flows by solving the Falkner--Skan boundary layer. We then apply PINNs for the simulation of four turbulent-flow cases, i.e., zero-pressure-gradient boundary layer, adverse-pressure-gradient boundary layer, and turbulent flows over a NACA4412 airfoil and the periodic hill. Our results show the excellent applicability of PINNs for laminar flows with strong pressure gradients, where predictions with less than 1\% error can be obtained. For turbulent flows, we also obtain very good accuracy on simulation results even for the Reynolds-stress components.},
	annote = {Extracted Annotations (2/21/2024, 12:38:10 PM)
"without any specific model or assumption for turbulence and by taking only the data on the domain boundaries" (Eivazi et al 2022:75118)"he solution vector of the PDE system uðt; xÞ is the output." (Eivazi et al 2022:75119)"general spatiotemporal partial differential equation as ut þN u¼ 0" (Eivazi et al 2022:75119)"the approximation of the solution by the MLP as{\textasciitilde}u ¼ fðt; xÞ" (Eivazi et al 2022:75119)"By applying the chain rule on f, the derivatives with respect to space and time can be obtained" (Eivazi et al 2022:75119)"X andW as the input and output spaces and each pair of vectors ðx;yÞ" (Eivazi et al 2022:75119)"optimize the parameters of the neural network, i.e., weightsw and biasesb, to approximate the mapping f: X !W such that a loss function LðfðxÞ;yÞ is minimized." (Eivazi et al 2022:75119)"In PINNs, the temporal and the spatial coordinatesðt; xÞ are the inputs of the MLP," (Eivazi et al 2022:75119)"x 2X" (Eivazi et al 2022:75119)"t 2 0; T" (Eivazi et al 2022:75119)"ut is its derivative with respect to time t" (Eivazi et al 2022:75119)"N½ denotes a nonlinear differential operator." (Eivazi et al 2022:75119)"Let us consider the residual of the partial differential equations eðt; xÞ as the left-hand side" (Eivazi et al 2022:75119)"e:¼ ut þN u" (Eivazi et al 2022:75119)"The training process of a PINN consists of both supervised and unsupervised parts." (Eivazi et al 2022:75120)"Following the work by Raissi et al.,24 we implement a full-batch training" (Eivazi et al 2022:75120)"continue the training using the limited-memory BroydenFletcher-Goldfarb-Shanno (L-BFGS) algorithm43 to obtain the solution." (Eivazi et al 2022:75120)read (note on p.75120)"outputs are the mean" (Eivazi et al 2022:75120)"FSBL" (Eivazi et al 2022:75121)"Relative'2-norm of errors" (Eivazi et al 2022:75121)"U" (Eivazi et al 2022:75121)"V" (Eivazi et al 2022:75121)"P" (Eivazi et al 2022:75121)"streamwise and wall-normal components of velocity" (Eivazi et al 2022:75121)"pressure" (Eivazi et al 2022:75121)"RCT" (Eivazi et al 2022:75121)"and Reynolds-stress components (u2; uv, and v2)" (Eivazi et al 2022:75121)"In our setup, only the data on the domain boundaries are used as the training dataset" (Eivazi et al 2022:75121)"total loss is the summation of the supervised loss and the residual" (Eivazi et al 2022:75121)"sents the given data for point n on the boundaries" (Eivazi et al 2022:75121)"1" (Eivazi et al 2022:75121)"To evaluate the accuracy of the predictions, we consider the relative'2-norm of errors Ei on all the computational points and for the ith variable" (Eivazi et al 2022:75121)"repre-" (Eivazi et al 2022:75121)"reference data is obtained from the analytical solution on a grid with the resolution of ðNx; NyÞ¼ð501; 201Þ" (Eivazi et al 2022:75122)"MLP comprises 8 hidden layers, each containing 20 neurons with hyperbolic tangent function (tanh) as the activation function." (Eivazi et al 2022:75122)"and then apply Broyden-Fletcher-Goldfarb-Shanno (BFGS) algorithm to obtain a more accurate solution" (Eivazi et al 2022:75122)},
	author = {Eivazi, Hamidreza and Tahani, Mojtaba and Schlatter, Philipp and Vinuesa, Ricardo},
	date-added = {2024-03-13 18:25:48 +0100},
	date-modified = {2024-03-13 18:25:48 +0100},
	doi = {10.1063/5.0095270},
	file = {Eivazi et al_2022_Physics-informed neural networks for solving Reynolds-averaged Navier--Stokes.pdf:/Users/marcelloceresini/Zotero/storage/RWEGZUSJ/Eivazi et al_2022_Physics-informed neural networks for solving Reynolds-averaged Navier--Stokes.pdf:application/pdf;Snapshot:/Users/marcelloceresini/Zotero/storage/A8M9BCRJ/2847279.html:text/html},
	issn = {1070-6631},
	journal = {Physics of Fluids},
	keywords = {notion, ALREADY READ},
	month = jul,
	number = {7},
	pages = {075117},
	title = {Physics-informed neural networks for solving {Reynolds}-averaged {Navier}--{Stokes} equations},
	url = {https://doi.org/10.1063/5.0095270},
	urldate = {2024-02-20},
	volume = {34},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1063/5.0095270}}

@article{jin_nsfnets_2021,
	abstract = {In the last 50 years there has been a tremendous progress in solving numerically the Navier-Stokes equations using finite differences, finite elements, spectral, and even meshless methods. Yet, in many real cases, we still cannot incorporate seamlessly (multi-fidelity) data into existing algorithms, and for industrial-complexity applications the mesh generation is time consuming and still an art. Moreover, solving ill-posed problems (e.g., lacking boundary conditions) or inverse problems is often prohibitively expensive and requires different formulations and new computer codes. Here, we employ physics-informed neural networks (PINNs), encoding the governing equations directly into the deep neural network via automatic differentiation, to overcome some of the aforementioned limitations for simulating incompressible laminar and turbulent flows. We develop the Navier-Stokes flow nets (NSFnets) by considering two different mathematical formulations of the Navier-Stokes equations: the velocity-pressure (VP) formulation and the vorticity-velocity (VV) formulation. Since this is a new approach, we first select some standard benchmark problems to assess the accuracy, convergence rate, computational cost and flexibility of NSFnets; analytical solutions and direct numerical simulation (DNS) databases provide proper initial and boundary conditions for the NSFnet simulations. The spatial and temporal coordinates are the inputs of the NSFnets, while the instantaneous velocity and pressure fields are the outputs for the VP-NSFnet, and the instantaneous velocity and vorticity fields are the outputs for the VV-NSFnet. This is unsupervised learning and, hence, no labeled data are required beyond boundary and initial conditions and the fluid properties. The residuals of the VP or VV governing equations, together with the initial and boundary conditions, are embedded into the loss function of the NSFnets. No data is provided for the pressure to the VP-NSFnet, which is a hidden state and is obtained via the incompressibility constraint without extra computational cost. Unlike the traditional numerical methods, NSFnets inherit the properties of neural networks (NNs), hence the total error is composed of the approximation, the optimization, and the generalization errors. Here, we empirically attempt to quantify these errors by varying the sampling (``residual'') points, the iterative solvers, and the size of the NN architecture. For the laminar flow solutions, we show that both the VP and the VV formulations are comparable in accuracy but their best performance corresponds to different NN architectures. The initial convergence rate is fast but the error eventually saturates to a plateau due to the dominance of the optimization error. For the turbulent channel flow, we show that NSFnets can sustain turbulence at Reτ∼1,000, but due to expensive training we only consider part of the channel domain and enforce velocity boundary conditions on the subdomain boundaries provided by the DNS data base. We also perform a systematic study on the weights used in the loss function for balancing the data and physics components, and investigate a new way of computing the weights dynamically to accelerate training and enhance accuracy. In the last part, we demonstrate how NSFnets should be used in practice, namely for ill-posed problems with incomplete or noisy boundary conditions as well as for inverse problems. We obtain reasonably accurate solutions for such cases as well without the need to change the NSFnets and at the same computational cost as in the forward well-posed problems. We also present a simple example of transfer learning that will aid in accelerating the training of NSFnets for different parameter settings.},
	annote = {Extracted Annotations (2/21/2024, 12:25:37 PM)
"velocity-pressure (VP) formulation" (Jin et al 2021:109951)"vorticity-velocity (VV) formulation" (Jin et al 2021:109951)"Analytical solutions and direct numerical simulation (DNS) databases provide proper initial and boundary conditions for the NSFnet simulations" (Jin et al 2021:109951)"instantaneous velocity and pressure elds are the outputs for the" (Jin et al 2021:109951)"VP" (Jin et al 2021:109951)"instantaneous velocity and vorticity elds are the outputs for the" (Jin et al 2021:109951)"No data is provided for the pressure" (Jin et al 2021:109951)"VV-" (Jin et al 2021:109951)"systematic study on the weights used in the loss function for the data/physics components and investigate a new way of computing the weights dynamically to accelerate training and enhance accuracy" (Jin et al 2021:109952)"unsteady incompressible three-dimensional Navier-Stokes equation" (Jin et al 2021:109955)"VP form" (Jin et al 2021:109955)"u(x; t) = [u; v; w]T is the non-dimensional velocity vector, p is the non-dimensional pressure" (Jin et al 2021:109955)"Re = Uref Dref = is the Reynolds number" (Jin et al 2021:109955)"D and N denote the Dirichlet and Neumann boundaries" (Jin et al 2021:109955)"nonlinear activation function is the hyper tangent function tanh" (Jin et al 2021:109956)"(k+1) = (k) rLe rLb rLi" (Jin et al 2021:109959)"estimates of and can be computed by" (Jin et al 2021:109959)"max fjrLe jg is the maximum value attained by jrLe j" (Jin et al 2021:109959)"jr(k) Lb j and jr(k) Li j denote the means of jr(k) Lb j and jr(k) Li j, respectively" (Jin et al 2021:109959)"alternative" (Jin et al 2021:109959)"weighting coecients for the next iteration are updated using a moving average" (Jin et al 2021:109959)" = 0:1" (Jin et al 2021:109960)"L2 error at each time step as V =k {\textasciicircum} V k2 = k V k2 ; (9) where V denotes the velocity components (u; v; w) or the pressure p" (Jin et al 2021:109960)},
	author = {Jin, Xiaowei and Cai, Shengze and Li, Hui and Karniadakis, George Em},
	date-added = {2024-03-13 18:25:48 +0100},
	date-modified = {2024-03-13 18:25:48 +0100},
	doi = {10.1016/j.jcp.2020.109951},
	file = {Jin et al_2021_NSFnets (Navier-Stokes flow nets).pdf:/Users/marcelloceresini/Zotero/storage/Y2LMHDK8/Jin et al_2021_NSFnets (Navier-Stokes flow nets).pdf:application/pdf;ScienceDirect Snapshot:/Users/marcelloceresini/Zotero/storage/QKYWHCUH/S0021999120307257.html:text/html},
	issn = {0021-9991},
	journal = {Journal of Computational Physics},
	keywords = {notion, ALREADY READ, Ill-posed problems, PINNs, Transfer learning, Turbulence, Velocity-pressure formulation, Vorticity-velocity formulation},
	month = feb,
	pages = {109951},
	shorttitle = {{NSFnets} ({Navier}-{Stokes} flow nets)},
	title = {{NSFnets} ({Navier}-{Stokes} flow nets): {Physics}-informed neural networks for the incompressible {Navier}-{Stokes} equations},
	url = {https://www.sciencedirect.com/science/article/pii/S0021999120307257},
	urldate = {2024-02-20},
	volume = {426},
	year = {2021},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0021999120307257},
	bdsk-url-2 = {https://doi.org/10.1016/j.jcp.2020.109951}}

@article{xiao_physics-informed_2023,
	abstract = {Recently, Physics-informed neural networks (PINNs) have proven to be an efficient machine-learning method for solving partial differential equations. However, this method can be quite challenging when solving complex problems with shock/material discontinuities or multi-scale features, such as turbulence. In this paper, we propose an improved PINNs framework for solving the Reynolds-averaged Navier--Stokes (RANS) equations for turbulent mixing induced by the Rayleigh--Taylor (RT) instability. The RANS model is based on the closure form of the K--L model. However, the transport equations of the turbulent kinetic energy K and turbulent length scale L are not included and are instead predicted directly by neural networks, thus resulting in an inverse problem. Several modifications are made to the original PINNs to improve its applicability to RT turbulent mixing and accelerate the training optimization process. We first examine the applicability of the PINNs for solving multi-material Euler equations without considering turbulence. Then, PINNs is applied to the RT turbulent mixing problem using training data from the traditional K--L model. The results confirm the ability of the PINNs to predict the entire spatiotemporal field using limited training data. Next, we further train the PINNs using data from the implicit large eddy simulation (ILES), which yields a PINN-based turbulence model that performs better than the traditional K--L model. These results shed light on further applications of PINNs for complex problems, particularly those with limited measurement data and unknown physical models.},
	annote = {Extracted Annotations (2/21/2024, 12:20:31 PM)
"traditional K-L model" (Xiao et al 2023:106025)"(RANS) equations for turbulent mixing induced by the Rayleigh-Taylor (RT) instability. The RANS model is based on the closure form of the K-L model." (Xiao et al 2023:106025)"transport equations of the turbulent kinetic energy K and turbulent length scale L are not included and are instead predicted directly by neural networks, thus resulting in an inverse problem." (Xiao et al 2023:106025)"further train the PINNs using data from the implicit large eddy simulation (ILES), which yields a PINN-based turbulence model that performs better than the traditional K-L model" (Xiao et al 2023:106025)},
	author = {Xiao, Meng-Juan and Yu, Teng-Chao and Zhang, You-Sheng and Yong, Heng},
	date-added = {2024-03-13 18:25:48 +0100},
	date-modified = {2024-03-13 18:25:48 +0100},
	doi = {10.1016/j.compfluid.2023.106025},
	file = {ScienceDirect Snapshot:/Users/marcelloceresini/Zotero/storage/CD699BLL/S0045793023002505.html:text/html;Xiao et al_2023_Physics-informed neural networks for the Reynolds-Averaged Navier--Stokes.pdf:/Users/marcelloceresini/Zotero/storage/NLNYC9KH/Xiao et al_2023_Physics-informed neural networks for the Reynolds-Averaged Navier--Stokes.pdf:application/pdf},
	issn = {0045-7930},
	journal = {Computers \& Fluids},
	keywords = {notion, ALREADY READ, PINNs, K--L model, Rayleigh--Taylor, Turbulent mixing},
	month = nov,
	pages = {106025},
	title = {Physics-informed neural networks for the {Reynolds}-{Averaged} {Navier}--{Stokes} modeling of {Rayleigh}--{Taylor} turbulent mixing},
	url = {https://www.sciencedirect.com/science/article/pii/S0045793023002505},
	urldate = {2024-02-20},
	volume = {266},
	year = {2023},
	bdsk-url-1 = {https://www.sciencedirect.com/science/article/pii/S0045793023002505},
	bdsk-url-2 = {https://doi.org/10.1016/j.compfluid.2023.106025}}

@article{gladstone_mesh-based_2024,
	abstract = {Abstract
            Physics-based deep learning frameworks have shown to be effective in accurately modeling the dynamics of complex physical systems with generalization capability across problem inputs. However, time-independent problems pose the challenge of requiring long-range exchange of information across the computational domain for obtaining accurate predictions. In the context of graph neural networks (GNNs), this calls for deeper networks, which, in turn, may compromise or slow down the training process. In this work, we present two GNN architectures to overcome this challenge---the edge augmented GNN and the multi-GNN. We show that both these networks perform significantly better than baseline methods, such as MeshGraphNets, when applied to time-independent solid mechanics problems. Furthermore, the proposed architectures generalize well to unseen domains, boundary conditions, and materials. Here, the treatment of variable domains is facilitated by a novel coordinate transformation that enables rotation and translation invariance. By broadening the range of problems that neural operators based on graph neural networks can tackle, this paper provides the groundwork for their application to complex scientific and industrial settings.},
	annote = {Extracted Annotations (3/6/2024, 12:19:43 PM)
"time‑independent problems pose the challenge of requiring long‑range exchange of information across the computational domain for obtaining accurate predictions" (Gladstone et al 2024:3394)"this calls for deeper networks, which, in turn, may compromise or slow down the training process" (Gladstone et al 2024:3394)"edge augmented GNN and the multi‑GNN. We show that both these networks perform significantly better than baseline methods, such as MeshGraphNets, when applied to time‑independent solid mechanics problems" (Gladstone et al 2024:3394)"the treatment of variable domains is facilitated by a novel coordinate transformation that enables rotation and translation invariance" (Gladstone et al 2024:3394)"While these surrogates are useful to find the specific solution of a PDE with a given set of parameters, a slight modification of such parameters, boundary conditions, or domain geometry requires re-training, making them less attractive in settings such as optimization, design, and uncertainty quantification." (Gladstone et al 2024:3394)"With the purpose of obtaining resolution invariance, a class of neural network-based PDE solvers focuses on continuous graph representation of the computational domain" (Gladstone et al 2024:3394)"In such representations, the continuous nature of the network update guarantees the invariance of the architecture with respect to the resolution of the data and the enhanced interaction among nodes, typical of graph-based networks, improves the accuracy of the architecture for complex physical systems" (Gladstone et al 2024:3394)"In this work we explore mesh-based GNN architectures; the latter exploit local (or short-range) interactions between neighboring nodes to estimate the response" (Gladstone et al 2024:3395)"As most physical systems involve local interactions, GNN-based surrogates are among the best candidates to serve as effective simulator engines" (Gladstone et al 2024:3395)"This happens because in general the exchange of information between distant nodes requires a large number of message passing steps" (Gladstone et al 2024:3395)"propose two GNN architectures that overcome the challenge of longrange message passing without using a deep network" (Gladstone et al 2024:3395)"the Edge Augmented Graph Neural Network (EA-GNN) and the multi-graph neural network (M-GNN)" (Gladstone et al 2024:3395)"we introduce an invariant simulation coordinate space by moving the physical systems to a simulation space that is invariant to translation and rotation" (Gladstone et al 2024:3395)"we instead add edges to increase connectivity between nodes that are far apart" (Gladstone et al 2024:3395)"To avoid increasing the depth of the network, we increase the non-locality of the graph by introducing augmented edges between randomly selected nodes (see Fig. 1c)" (Gladstone et al 2024:3396)"we apply the multigrid concept to graphs and pass information across the computational domain in a hierarchical manner" (Gladstone et al 2024:3397)"resembles the graph U-Net architecture used in28 for image processing tasks such as image classification and segmentation" (Gladstone et al 2024:3397)"we select a set of nodes from the graph and, if no edges exist between them, we add a bi-directional edge." (Gladstone et al 2024:3397)"since this edge is not similar to other edges, we add an extra feature to the edge attributes to indicate that the edge belongs to the augmentation set." (Gladstone et al 2024:3397)"number of additional edges in the augmented graph is a hyperparameter" (Gladstone et al 2024:3397)"vertices of augmented edges are selected randomly, by following a uniform distribution" (Gladstone et al 2024:3397)"more sophisticated sampling methods, such as farthest point sampling54, did not yield significant improvements in the performance of the model, we employed random sampling of nodes in all of our experiments" (Gladstone et al 2024:3397)"with the purpose of making the graph translation and rotation invariant, before the graph is created, the mesh is transformed to be in the principal axis coordinate system (referred to as simulation coordinate system)" (Gladstone et al 2024:3397)"The node attributes are nodal positions in simulation coordinates (x, y), nodal type (interior or boundary), type of the boundary condition applied (Dirichlet homogenous and non-homogenous, Neumann), direction and" (Gladstone et al 2024:3397)"magnitude of the boundary conditions, a flag for body forces and its magnitude and direction all in simulation coordinate system. The edge attributes are the Euclidean distance between the nodes, and positional difference in x and y directions, i.e., x=x2−x1 and y=y2−y1." (Gladstone et al 2024:3398)"Multi‑graph neural network (M‑GNN)" (Gladstone et al 2024:3398)"three components: (1) an encoder, (2) an M-Net block, and (3) a decoder" (Gladstone et al 2024:3398)"here we do not consider edge attributes" (Gladstone et al 2024:3398)"The updated node attributes, u′ , are then passed through a series of alternating layers of down-sampling operations and m-GN blocks" (Gladstone et al 2024:3398)"This is followed by a series of alternating up-sampling operations and m-GN blocks." (Gladstone et al 2024:3398)"For the adaptive selection of nodes, we use the "U-net sub-sampling" algorithm28." (Gladstone et al 2024:3398)"Specifically, the node attributes are projected onto the trainable vector p using the scalar projection uTp and top k nodes are selected based on the projected values" (Gladstone et al 2024:3398)"information retained by node i when projected onto p , sampling the top k nodes ensures that the smaller graph retains the maximum information." (Gladstone et al 2024:3398)"The up-sampling operation up-samples the graph by recording the locations of nodes selected in the corresponding down-sampling layer and uses this information to place the nodes back to their original positions in the graph." (Gladstone et al 2024:3398)"In order to make sure that there are no disconnected nodes after down-sampling, as well as to improve the N mij) (1) i are connectivity between the nodes, we compute the lth graph power, similarly to28, and use the resulting graph. This operation builds links between nodes which are at-most l hops away in the graph. This could be done by multiplying the adjacency matrix of the graph by itself l times." (Gladstone et al 2024:3398)"Since the nodal positions are considered as node attributes, the model can calculate the edge attributes such as Euclidean distance and positional difference indirectly from the node attributes. Thus no information is lost due to the removal of edge attributes." (Gladstone et al 2024:3399)"To resolve this issue, we use group equivariance as our inductive bias where we ensure the graph is invariant to translation and rotation by transforming the geometry into the principal axis coordinate system." (Gladstone et al 2024:3399)"First, the coordinates are made translation invariant by moving the center of the original coordinate system to the centroid of the graph" (Gladstone et al 2024:3399)"n=1 by rotating the coordinate system to principal axes." (Gladstone et al 2024:3399)"This is done by calculating eigenvalues and eigenvectors of the matrix XTXc" (Gladstone et al 2024:3399)"Data augmentation" (Gladstone et al 2024:3399)"we use both the Delaunay and the "Packing of Parallelograms" algorithms for mesh generation. Second, we add noise to nodal coordinates" (Gladstone et al 2024:3399)"total number of edges after augmentation is Ne+Aperc×Ne" (Gladstone et al 2024:3400)"Aperc=20\% for our experiments" (Gladstone et al 2024:3400)"A total of 6 GN blocks are added" (Gladstone et al 2024:3400)"The parameters of all the four functions ( χ , φ , γ , β ) are shared across all the GN blocks. After each GN block, a skip connection is added as shown in Eq. (2)" (Gladstone et al 2024:3400)"We use a dropout layer after the encoder and between each GN block with a dropout percentage of 0.1 to reduce overfitting." (Gladstone et al 2024:3400)"a) connectiv‑ ity enhancement---the connectivity of the graph is enhanced by connecting the nodes that are 3 hops away. This is done by multiplying the adjacency matrix, A by itself twice, i.e. Aupd=A×A×A . This ensures that we do not have any disconnected nodes i.e., all the nodes are connected to at least one neighbor" (Gladstone et al 2024:3400)"Down‑sampling---a layer that projects the node attributes onto a one-dimensional, trainable projection vector p , and samples k nodes based on the projected values. The value of k is determined by the down-sampling ratio, r. For the experiments, we have set r=0.6" (Gladstone et al 2024:3400)"For all the datasets, there are, on average, 1100 nodes per graph/mesh." (Gladstone et al 2024:3401)"scaled mean absolute error loss function L. The scaling depends on a combination of the boundary conditions associated with each sample" (Gladstone et al 2024:3401)"�dn�ℓ1+�nn�ℓ1)�yn−n�ℓ1" (Gladstone et al 2024:3401)"dn and nn are the Dirichlet loading and Neumann displacement vectors of the n-th sample" (Gladstone et al 2024:3401)},
	author = {Gladstone, Rini Jasmine and Rahmani, Helia and Suryakumar, Vishvas and Meidani, Hadi and D'Elia, Marta and Zareei, Ahmad},
	date-added = {2024-03-13 18:25:48 +0100},
	date-modified = {2024-03-13 18:25:48 +0100},
	doi = {10.1038/s41598-024-53185-y},
	file = {Gladstone et al_2024_Mesh-based GNN surrogates for time-independent PDEs.pdf:/Users/marcelloceresini/Zotero/storage/54IVUR7Y/Gladstone et al_2024_Mesh-based GNN surrogates for time-independent PDEs.pdf:application/pdf},
	issn = {2045-2322},
	journal = {Scientific Reports},
	keywords = {notion, ALREADY READ},
	language = {en},
	month = feb,
	number = {1},
	pages = {3394},
	title = {Mesh-based {GNN} surrogates for time-independent {PDEs}},
	url = {https://www.nature.com/articles/s41598-024-53185-y},
	urldate = {2024-03-06},
	volume = {14},
	year = {2024},
	bdsk-url-1 = {https://www.nature.com/articles/s41598-024-53185-y},
	bdsk-url-2 = {https://doi.org/10.1038/s41598-024-53185-y}}



@Book{Doe2015,
	author  = {Jon Doe},
	title   = {What do you think about Jon Doe},
	date    = {2015-01-02},
	editor  = {Musterverlag},
	url     = {http://localhost:8080},
	urldate = {2017-02-16},
}

@Online{Bloggs2013,
	author  = {Joe Bloggs},
	editor  = {Blog},
	title   = {Lorem ipsum dolor sit amet.},
	date    = {2013-08-01},
	url     = {http://www.loremipsum.de/},
	urldate = {2017-02-16},
}

@Comment{jabref-meta: databaseType:biblatex;}
